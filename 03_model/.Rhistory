############################################################
# Data Challenge competition
# Budapest, February 2023
# Author: Team Bella Vita
# Aim of this code: To build a predictive models to predict the outcome of football matches using historical data.
# Note:
# - Data was provided by the organizer of the competition. It is found to be clean, no missing values, no duplicates.
# - There are 23 divisions and 10 seasons in the data.
############################################################
# BASIC SETUP
############################################################
# empty memory
rm(list=ls())
# KOMMENT: install library if not exist -> megoldani majd
# csekkolni melyik kell ezek közül
# Load libraries
library(tidyverse) # imap function for roc curve
# library(ggplot2)
# library(ggthemes)
# library(grid)
# library(ggplot2)
# library(lattice)
# library(glmnet)
# install.packages("data.table")
library("data.table") # to use data in an efficient way
# empty memory
rm(list=ls())
library("tidyverse") # imap function for roc curve
library("data.table") # to use data in an efficient way
library("data.table") # to use data in an efficient way
install.packages("data.table")
library("data.table") # to use data in an efficient way
library("data.table") # to use data in an efficient way
install.packages("data.table")
library("data.table") # to use data in an efficient way
library("readxl") # to read xlsx data
library("caret") # to split data into train and holdput sets
library("dplyr") # to append commands easier
# library("skimr")
# library(gridExtra)
library("ROCR") # to evaluate model performance
# Set the working directory
dir <-  "~/Library/CloudStorage/GoogleDrive-gulyati@gmail.com/My Drive/DataChallenge/"
#location folders
data_in <- paste0(dir,"01_raw/")
# func <- paste0(dir, "work/") - nem használt
output <- paste0(dir, "04_output/")
data <- as.data.table(read_excel(paste0(data_in, "competition_table.xlsx")))
# BASIC STISTICS
message(paste0("Döntetlen aránya: ", round(sum(data$draw_flag) / nrow(data), digits = 2)))
message(paste0("Hazai győzelem aránya: ", round(sum(data$home_win_flag) / nrow(data), digits = 2)))
message(paste0("Vendég győzelem aránya: ", round(sum(data$away_win_flag) / nrow(data), digits = 2)))
data <- data %>%
mutate(home_win_flag = factor(ifelse(home_win_flag == 0, "no", "yes"), levels = c("no", "yes")))
data <- data %>%
mutate(away_win_flag = factor(ifelse(away_win_flag == 0, "no", "yes"), levels = c("no", "yes")))
data <- data %>%
mutate(draw_flag = factor(ifelse(draw_flag == 0, "no", "yes"), levels = c("no", "yes")))
# data structure
str(data)
set.seed(1990)
# smaller than usual training set so that models run faster
train_indices <- createDataPartition(data$home_win_flag, p = 0.7, list = FALSE)
data_train <- data[train_indices, ]
data_holdout <- data[-train_indices, ]
dim(data_train)
dim(data_holdout)
#######################################################
# predictors_1 <- c(basic_numeric_vars)
predictors_1 <- colnames(data[,-c("home_win_flag", "draw_flag", "away_win_flag")])
train_control <- trainControl(
method = "cv",
n = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary  # necessary!
)
install.packages("h2o")
# AutoML-hez
# install.packages("h2o")
library(h2o)
h2o.init(nthreads=-1)
h2o.init()
h2o.init(nthreads = -1, max_mem_size = '2g', ip = "127.0.0.1", port = 54321)
h2o.init(nthreads = 1, max_mem_size = '2g', ip = "127.0.0.1", port = 54321)
library(caretEnsemble)
install.packages("caretEnsemble")
library(caretEnsemble)
# trainControl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
automl_model <- trainAuto(x = data_train[, predictors_1], y = data_train$home_win_flag, trControl = train_control, metric = "auc", maximize = FALSE)
library(caret)
library(caretEnsemble)
# trainControl <- trainControl(method = "repeatedcv", number = 10, repeats = 3)
automl_model <- trainAuto(x = data_train[, predictors_1], y = data_train$home_win_flag, trControl = train_control, metric = "auc", maximize = FALSE)
?train
install.packages("bnclassify")
# Naive Bayes Classifier
library("bnclassify")
#######################################################
predictors_1 <- c(basic_numeric_vars)
# Basic numeric variables
basic_numeric_vars <- c(
"odds_home_team_win",
"odds_away_team_win",
"odds_draw"
)
#######################################################
predictors_1 <- c(basic_numeric_vars)
rf_model_1 <- train(
formula(paste0("home_win_flag ~", paste0(predictors_1, collapse = " + "))),
tuneLength = 1,
data = data_train,
method = 'nbDiscrete',
na.action = na.omit,
importance = 'permutation',
# mtry: cannot be more than the number of predictors
tuneGrid = tune_grid,
trControl = train_control)
# rf_model_1
tune_grid <- expand.grid(
.mtry = 3,
.splitrule = "gini",
.min.node.size = c(5) # implicitly sets the depth of your trees
)
rf_model_1 <- train(
formula(paste0("home_win_flag ~", paste0(predictors_1, collapse = " + "))),
tuneLength = 1,
data = data_train,
method = 'nbDiscrete',
na.action = na.omit,
importance = 'permutation',
# mtry: cannot be more than the number of predictors
tuneGrid = tune_grid,
trControl = train_control)
tune_grid <- expand.grid(
.mtry = 3,
.splitrule = "gini"
# .min.node.size = c(5) # implicitly sets the depth of your trees
)
nbc_model <- train(
formula(paste0("home_win_flag ~", paste0(predictors_1, collapse = " + "))),
tuneLength = 1,
data = data_train,
method = 'nbDiscrete',
na.action = na.omit,
importance = 'permutation',
# mtry: cannot be more than the number of predictors
tuneGrid = tune_grid,
trControl = train_control)
tune_grid <- expand.grid(
.mtry = 3,
.splitrule = "gini",
smooth = 1
# .min.node.size = c(5) # implicitly sets the depth of your trees
)
nbc_model <- train(
formula(paste0("home_win_flag ~", paste0(predictors_1, collapse = " + "))),
tuneLength = 1,
data = data_train,
method = 'nbDiscrete',
na.action = na.omit,
importance = 'permutation',
# mtry: cannot be more than the number of predictors
tuneGrid = tune_grid,
trControl = train_control)
tune_grid <- expand.grid(
.mtry = 3,
.splitrule = "gini",
smooth = 1,
.min.node.size = c(5) # implicitly sets the depth of your trees
)
nbc_model <- train(
formula(paste0("home_win_flag ~", paste0(predictors_1, collapse = " + "))),
tuneLength = 1,
data = data_train,
method = 'nbDiscrete',
na.action = na.omit,
importance = 'permutation',
# mtry: cannot be more than the number of predictors
tuneGrid = tune_grid,
trControl = train_control)
tune_grid <- expand.grid(
.mtry = 3,
.splitrule = "gini",
.min.node.size = c(5) # implicitly sets the depth of your trees
)
bayesridge_model <- train(
formula(paste0("home_win_flag ~", paste0(predictors_1, collapse = " + "))),
tuneLength = 1,
data = data_train,
method = 'bridge',
na.action = na.omit,
importance = 'permutation',
# mtry: cannot be more than the number of predictors
tuneGrid = tune_grid,
trControl = train_control)
library("monomvn")
bayesridge_model <- train(
formula(paste0("home_win_flag ~", paste0(predictors_1, collapse = " + "))),
tuneLength = 1,
data = data_train,
method = 'bridge',
na.action = na.omit,
importance = 'permutation',
# mtry: cannot be more than the number of predictors
tuneGrid = tune_grid,
trControl = train_control)
predictors_1
bayesridge_model <- train(
formula(paste0("home_win_flag ~", paste0(predictors_1, collapse = " + "))),
tuneLength = 1,
data = data_train,
method = 'blassoAveraged',
na.action = na.omit,
importance = 'permutation',
# mtry: cannot be more than the number of predictors
tuneGrid = tune_grid,
trControl = train_control)
bayesridge_model <- train(
formula(paste0("home_win_flag ~", paste0(predictors_1, collapse = " + "))),
tuneLength = 1,
data = data_train,
method = 'bayesglm',
na.action = na.omit,
importance = 'permutation',
# mtry: cannot be more than the number of predictors
tuneGrid = tune_grid,
trControl = train_control)
library("arm")
bayesridge_model <- train(
formula(paste0("home_win_flag ~", paste0(predictors_1, collapse = " + "))),
tuneLength = 1,
data = data_train,
method = 'bayesglm',
na.action = na.omit,
importance = 'permutation',
# mtry: cannot be more than the number of predictors
tuneGrid = tune_grid,
trControl = train_control)
bayesridge_model <- train(
formula(paste0("home_win_flag ~", paste0(predictors_1, collapse = " + "))),
tuneLength = 1,
data = data_train,
method = 'bayesglm',
na.action = na.omit,
importance = 'permutation',
# mtry: cannot be more than the number of predictors
# tuneGrid = tune_grid,
trControl = train_control)
bayesridge_model <- train(
formula(paste0("home_win_flag ~", paste0(predictors_1, collapse = " + "))),
tuneLength = 1,
data = data_train,
method = 'bayesglm',
na.action = na.omit,
importance = 'permutation',
# mtry: cannot be more than the number of predictors
# tuneGrid = tune_grid,
trControl = train_control)
bayesridge_model <- train(
formula(paste0("home_win_flag ~", paste0(predictors_1, collapse = " + "))),
tuneLength = 1,
data = data_train,
method = 'bayesglm',
na.action = na.omit,
importance = 'permutation',
# mtry: cannot be more than the number of predictors
# tuneGrid = tune_grid,
trControl = train_control)
set.seed(857)
set.seed(857)
glm_model <- train(home_win_flag ~ predictors_1,
method = "glm",
data = data_train,
trControl = train_control)
glm_model <- train("home_win_flag ~" predictors_1,
glm_model <- train(formula(paste0("home_win_flag ~", paste0(predictors_1, collapse = " + "))),
method = "glm",
data = data_train,
trControl = train_control)
# AUC on training setß
glm_model
varImp(glm_model)
varimp <- varImp(glm_model)
# variable importance
plot(varImp(glm_model[1:10]))
# variable importance
plot(varImp(glm_model[1:10]))
# AUC on training setß
glm_model
# AUC on holdout set
predicted_glm_holdout <- predict(glm_model,
newdata = data_holdout,
type = "prob")[["yes"]]
prediction_holdout_glm <- prediction(predicted_glm_holdout, data_holdout$home_win_flag)
auc_holdout_glm <- performance(prediction_holdout_glm, measure = "auc")@y.values[[1]]
message(paste0("AUC on holdout set for GLM: ", round(auc_holdout_glm, digits = 4)))
# compare model performance based on CV using AUC
summary(resamples(glm_model))
# compare model performance based on CV using AUC
summary(glm_model)
# the previously seen CV AUC
glm_model[["results"]][["ROC"]]
# AUC on training setß
glm_model
# the previously seen CV AUC
auc_cv <- glm_model[["results"]][["ROC"]]
message(paste0("AUC from CV: ", round(auc_cv, digits = 4)))
set.seed(19900829)
tune_grid <- expand.grid(
.nrounds = 3,
.max_depth = 2
)
xgb <- train(
formula(paste0("home_win_flag ~", paste0(predictors_1, collapse = " + "))),
tuneLength = 1,
data = data_train,
method = 'xgbTree',
na.action = na.omit,
importance = 'permutation',
# mtry: cannot be more than the number of predictors
tuneGrid = tune_grid,
trControl = train_control)
library("xgboost")
tune_grid <- expand.grid(
nrounds = c(100),  # this is n_estimators in the python code above
max_depth = c(10),
colsample_bytree = seq(0.5, 0.9, length.out = 5),
## The values below are default values in the sklearn-api.
eta = 0.1,
gamma=0,
min_child_weight = 1,
subsample = 1
)
xgb <- train(
formula(paste0("home_win_flag ~", paste0(predictors_1, collapse = " + "))),
tuneLength = 1,
data = data_train,
method = 'xgbTree',
na.action = na.omit,
importance = 'permutation',
# mtry: cannot be more than the number of predictors
tuneGrid = tune_grid,
trControl = train_control)
xgb
predictors_1 <- colnames(data[,-c("home_win_flag", "draw_flag", "away_win_flag")])
xgb <- train(
formula(paste0("home_win_flag ~", paste0(predictors_1, collapse = " + "))),
tuneLength = 1,
data = data_train,
method = 'xgbTree',
na.action = na.omit,
# importance = 'permutation',
# mtry: cannot be more than the number of predictors
tuneGrid = tune_grid,
trControl = train_control)
xgb
xgb
# Naive Bayes
# -------------------------------------------------
library(e1071)
# Create a preprocessing pipeline
preprocess <- preProcess(train_data, method = c("center", "scale"))
# Create a preprocessing pipeline
preprocess <- preProcess(data_train, method = c("center", "scale"))
# Apply the preprocessing to the training and testing sets
train_data_processed <- predict(preprocess, data_train)
test_data_processed <- predict(preprocess, data.test)
test_data_processed <- predict(preprocess, data_test)
test_data_processed <- predict(preprocess, data_holdout)
View(data_holdout)
View(prediction_holdout_glm)
# Define the control parameters for the train function
ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3,
classProbs = TRUE, summaryFunction = twoClassSummary)
# Train the Naive Bayes model using the train function
nb_model <- train(home_win_flag ~ ., data = train_data_processed, method = "naive_bayes",
trControl = ctrl, verbose = FALSE)
# Train the Naive Bayes model using the train function
nb_model <- train(formula(paste0("home_win_flag ~", paste0(predictors_1, collapse = " + "))),
method = "naive_bayes",
trControl = ctrl, verbose = FALSE)
train_data_processed
# Train the Naive Bayes model using the train function
nb_model <- train(home_win_flag ~ ., data = data_train[, predictors_1],
method = "naive_bayes",
trControl = ctrl, verbose = FALSE)
predictors_1
# Train the Naive Bayes model using the train function
nb_model <- train(home_win_flag ~ ., data = data_train[, c(predictors_1)],
method = "naive_bayes",
trControl = ctrl, verbose = FALSE)
# Train the Naive Bayes model using the train function
nb_model <- train("home_win_flag" ~ ., data = data_train[, c(predictors_1)],
method = "naive_bayes",
trControl = ctrl, verbose = FALSE)
# Train the Naive Bayes model using the train function
nb_model <- train(home_win_flag ~ ., data = data_train,
method = "naive_bayes",
trControl = ctrl, verbose = FALSE)
# Train the Naive Bayes model using the train function
nb_model <- train(home_win_flag ~ ., data = data_train[,-c("away_win_flag", "draw_flag")],
method = "naive_bayes",
trControl = ctrl, verbose = FALSE)
nb_model
max(data$season) %>% unique()
data$season %>% unique()
