# nullás osztás miatti hibás értékek javítása
data[,168:191] <- lapply(data[,168:191], function(x) {x[is.infinite(x)] <- 0; return(x)}) %>% as.data.table()
data[,168:191] <- lapply(data[,168:191], function(x) {x[is.na(x)] <- 0; return(x)}) %>% as.data.table()
set.seed(1990)
# smaller than usual training set so that models run faster
train_indices <- createDataPartition(data$home_win_flag, p = 0.7, list = FALSE)
data_train <- data[train_indices, ]
data_holdout <- data[-train_indices, ]
dim(data_train)
dim(data_holdout)
train_control <- trainControl(
method = "cv",
n = 5,
classProbs = TRUE,
summaryFunction = twoClassSummary  # necessary!
)
# MEGJ: repeated CV is egy megoldás
# Define the control parameters for the train function
# ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3,
#                      classProbs = TRUE, summaryFunction = twoClassSummary)
library(rstanarm)
# Fit Bayesian logistic regression model
stanglm_model <- stan_glm(home_win_flag ~ odds_home_team_win + odds_away_team_win,
data = data_train, family = binomial())
# Summarize the posterior distribution of coefficients
summary(stanglm_model)
# Plot the posterior distribution of coefficients
plot(stanglm_model)
predicted_stan_glm_holdout <- predict(stanglm_model,
newdata = data_holdout,
type = "prob")[["yes"]]
predicted_stan_glm_holdout <- predict(stanglm_model,
newdata = data_holdout,
type = "prob")[["yes"]]
# Summarize the posterior distribution of coefficients
summary(stanglm_model)
predicted_stan_glm_holdout <- predict(stanglm_model,
newdata = data_holdout,
type = "response")[["yes"]]
predicted_stan_glm_holdout <- predict(stanglm_model,
newdata = data_holdout,
type = "response")
prediction_holdout_stanglm <- prediction(predicted_stanglm_holdout, data_holdout$home_win_flag)
predicted_stan_glm_holdout <- predict(stanglm_model,
newdata = data_holdout)
predicted_stan_glm_holdout
prediction_holdout_stanglm <- prediction(predicted_stanglm_holdout, data_holdout$home_win_flag)
prediction_holdout_stanglm <- prediction(predicted_stanglm_holdout, data_holdout$home_win_flag)
str(predicted_stan_glm_holdout)
predicted_stan_glm_holdout <- predict(stanglm_model,
newdata = data_holdout)[["yes"]]
predicted_stan_glm_holdout <- predict(stanglm_model,
newdata = data_holdout)[["1"]]
prediction_holdout_stanglm <- prediction(predicted_stanglm_holdout, data_holdout$home_win_flag)
prediction_holdout_stanglm <- prediction(predicted_stan_glm_holdout, data_holdout$home_win_flag)
predicted_stan_glm_holdout <- predict(stanglm_model,
newdata = data_holdout)
prediction_holdout_stanglm <- prediction(predicted_stan_glm_holdout, data_holdout$home_win_flag)
auc_holdout_stanglm<- performance(prediction_holdout_stanglm, measure = "auc")@y.values[[1]]
message(paste0("AUC on holdout set for STAN GLM: ", round(auc_holdout_stanglm, digits = 4)))
# Create a stan_glm model
model <- stan_glm(formula = data_train[ ,colnames(data_train) %in% c("home_win_flag","odds_home_team_win", "odds_away_team_win"), with=FALSE],
data = data_train,
family = family,
prior_intercept = param$method$parameters$prior_intercept,
prior = param$method$parameters$prior,
chains = param$method$parameters$chains)
# Define custom model for stan_glm
stan_glm_model <- function(x, y, wts, param) {
# Extract formula and family from parameters
formula <- as.formula(param$method$parameters$formula)
family <- param$method$parameters$family
# Create a stan_glm model
model <- stan_glm(formula = data_train[ ,colnames(data_train) %in% c("home_win_flag","odds_home_team_win", "odds_away_team_win"), with=FALSE],
data = data_train,
family = family,
prior_intercept = param$method$parameters$prior_intercept,
prior = param$method$parameters$prior,
chains = param$method$parameters$chains)
# Return the fitted model object
return(model)
}
# Define cross-validation settings
train_control <- trainControl(
method = "cv",  # cross-validation method
number = 10,  # number of folds
verboseIter = TRUE,  # print progress to console
returnResamp = "all",  # return resampled data
savePredictions = "final"  # save final model predictions
)
# Define cross-validation settings
train_control <- trainControl(
method = "cv",  # cross-validation method
number = 5,  # number of folds
verboseIter = TRUE  # print progress to console
# returnResamp = "all",  # return resampled data
# savePredictions = "final"  # save final model predictions
)
# Define model settings and hyperparameters to be tuned
model <- "stan_glm"
tune_grid_stan_glm <- expand.grid(
family = list("binomial"),
prior_intercept = list(normal(0, 2)),
prior = list(normal(0, 2)),
chains = list(3)
)
# Define custom model for stan_glm
stan_glm_model <- function(x, y, wts, param) {
# Extract formula and family from parameters
formula <- as.formula(param$method$parameters$formula)
family <- param$method$parameters$family
# Create a stan_glm model
model <- stan_glm(formula = data_train[ ,colnames(data_train) %in% c("home_win_flag", "odds_away_team_win"), with=FALSE],
data = data_train,
family = family,
prior_intercept = param$method$parameters$prior_intercept,
prior = param$method$parameters$prior,
chains = param$method$parameters$chains)
# Return the fitted model object
return(model)
}
# Train the model using cross-validation
cv_results <- train(
x = data_train[, c("odds_home_team_win", "odds_away_team_win")],
y = factor(data_train[, "d_home_win_flag"]),
method = stan_glm_model,  # use custom model
trControl = train_control,
tuneGrid = tune_grid_stan_glm
)
# Define model settings and hyperparameters to be tuned
model <- "stan_glm"
tune_grid_stan_glm <- expand.grid(
family = list("binomial"),
prior_intercept = list(normal(0, 2)),
prior = list(normal(0, 2)),
chains = list(3)
)
# Train the model using cross-validation
cv_results <- train(
x = data_train[, c("odds_home_team_win")],
y = factor(data_train[, "d_home_win_flag"]),
method = stan_glm_model,  # use custom model
trControl = train_control,
tuneGrid = tune_grid_stan_glm
)
# Train the model using cross-validation
cv_results <- train(
x = data_train[, c("odds_home_team_win")],
y = factor(data_train[, "d_home_win_flag"]),
method = stan_glm_model,  # use custom model
trControl = train_control,
tuneGrid = tune_grid_stan_glm
)
# Train the model using cross-validation
cv_results <- train(
x = data_train[, c("odds_home_team_win")],
y = factor(data_train[, "home_win_flag"]),
method = stan_glm_model,  # use custom model
trControl = train_control,
tuneGrid = tune_grid_stan_glm
)
# Train the model using cross-validation
cv_results <- train(
x = data_train[, c("odds_home_team_win")],
y = factor(data_train[, "home_win_flag"]),
method = stan_glm_model,  # use custom model
trControl = train_control,
tuneGrid = tune_grid_stan_glm
)
# Train the model using cross-validation
cv_results <- train(
x = data_train[, c("odds_home_team_win")],
y = factor(data_train[, "home_win_flag"]),
method = model,  # use custom model
trControl = train_control,
tuneGrid = tune_grid_stan_glm
)
# Train the model using cross-validation
cv_results <- train(
x = data_train[, c("odds_home_team_win")],
y = factor(data_train[, "home_win_flag"]),
method = model,  # use custom model
trControl = train_control,
tuneGrid = tune_grid_stan_glm
)
# Train the model using cross-validation
cv_results <- train(
x = data_train[, c("odds_home_team_win")],
y = factor(data_train[, "home_win_flag"]),
method = "stan_glm",  # use custom model
trControl = train_control,
tuneGrid = tune_grid_stan_glm
)
# Define custom model for stan_glm
stan_glm_model <- function(x, y, wts, param) {
# Extract formula and family from parameters
formula <- as.formula(param$method$parameters$formula)
family <- param$method$parameters$family
# Create a stan_glm model
model <- stan_glm(formula = data_train[ ,colnames(data_train) %in% c("home_win_flag", "odds_away_team_win"), with=FALSE],
data = data_train,
family = family,
prior_intercept = param$method$parameters$prior_intercept,
prior = param$method$parameters$prior,
chains = param$method$parameters$chains)
# Return the fitted model object
return(model)
}
# Define model settings and hyperparameters to be tuned
model <- "stan_glm"
# Train the model using cross-validation
cv_results <- train(
x = data_train[, c("odds_home_team_win")],
y = factor(data_train[, "home_win_flag"]),
method = stan_glm_model,  # use custom model
trControl = train_control,
tuneGrid = tune_grid_stan_glm
)
# Define custom model for stan_glm
stan_glm_model <- function(x, y, param) {
# Extract formula and family from parameters
formula <- as.formula(param$method$parameters$formula)
family <- param$method$parameters$family
# Create a stan_glm model
model <- stan_glm(formula = data_train[ ,colnames(data_train) %in% c("home_win_flag", "odds_away_team_win"), with=FALSE],
data = data_train,
family = family,
prior_intercept = param$method$parameters$prior_intercept,
prior = param$method$parameters$prior,
chains = param$method$parameters$chains)
# Return the fitted model object
return(model)
}
# Train the model using cross-validation
cv_results <- train(
x = data_train[, c("odds_home_team_win")],
y = factor(data_train[, "home_win_flag"]),
method = stan_glm_model,  # use custom model
trControl = train_control,
tuneGrid = tune_grid_stan_glm
)
# another way
# Load required libraries
library(caret)
library(rstanarm)
# Define custom model function
stan_glm_model <- function(x, y, family, ...) {
fit <- stan_glm(x = x, y = y, family = family, ...)
return(fit)
}
# Add custom model to caret model list
models <- list(stan_glm = stan_glm_model)
# Define train control
train_control <- trainControl(method = "cv", number = 10)
# Define tuning grid
tune_grid <- expand.grid(family = c("binomial", "poisson"))
# Train model using caret
fit <- train(
x = data_train[, c("odds_home_team_win", "odds_away_team_win")],
y = data_train$d_home_win_flag,
method = "stan_glm",
trControl = train_control,
tuneGrid = tune_grid,
metric = "Accuracy",
verbose = FALSE,
allowParallel = TRUE
)
# another way
# Load required libraries
library(caret)
library(rstanarm)
# Define custom model function
stan_glm_model <- function(x, y, family, ...) {
fit <- stan_glm(x = x, y = y, family = family, ...)
return(fit)
}
# Add custom model to caret model list
models <- list(stan_glm = stan_glm_model)
# Define train control
train_control <- trainControl(method = "cv", number = 10)
# Define tuning grid
tune_grid <- expand.grid(family = c("binomial", "poisson"))
# Train model using caret
fit <- train(
x = data_train[, c("odds_home_team_win", "odds_away_team_win")],
y = data_train$d_home_win_flag,
method = "stan_glm",
trControl = train_control,
tuneGrid = tune_grid,
metric = "Accuracy",
verbose = FALSE,
allowParallel = TRUE
)
# Basic numeric variables
basic_numeric_vars <- c(
"odds_home_team_win",
"odds_away_team_win",
"odds_draw"
)
# glmnet (elastic net penalized) based selection
basic_numeric_vars_set2 <- c("odds_away_team_win",
"odds_home_team_win",
"home_team_expected_point_roll4_sum",
"away_team_expected_point_roll4_sum",
"away_team_expected_point_roll3_sum",
"away_team_expected_point_mean",
"home_team_corner_roll4_sum", # ?
"home_team_corner_roll3_sum", # ?
"home_team_corner_roll2_sum", # ?
"away_team_opponents_corner_roll4_sum", # ?
"home_team_opponents_corner_roll2_sum", # ?
"home_team_shot_roll2_sum", # ?
"away_team_opponents_shot_roll1_sum",  # ?
"away_team_opponents_shot_roll4_sum", # ?
"away_team_opponents_shot_roll3_sum", # ?
"away_team_opponents_shot_on_target_roll4_sum"
)
# xgb based selection
basic_numeric_vars_set3 <- c("odds_away_team_win",
"odds_home_team_win",
"odds_draw",
"odds_home_team_defeat_roll4_mean",
"odds_home_team_defeat_roll3_mean", # human input
"odds_home_team_defeat_roll2_mean",
"odds_away_team_defeat_roll4_mean",
"odds_away_team_defeat_roll3_mean", # human input
"odds_away_team_defeat_roll2_mean", # human input
"odds_away_team_win_roll4_mean",
"odds_away_team_win_roll3_mean",
"odds_home_team_win_roll4_mean", # human input
"odds_home_team_win_roll3_mean",
"odds_home_team_draw_roll4_mean",
"odds_home_team_draw_roll3_mean", # human input
"odds_away_team_draw_roll4_mean", # human input
"odds_away_team_draw_roll3_mean",
"away_team_expected_point_mean",
"home_team_expected_point_mean", # human input
"home_team_corner_roll3_sum", # ?
"away_team_corner_roll2_sum", # ?
"away_team_opponents_corner_roll4_sum", # ?
"home_team_shot_roll2_sum", # ?
"home_team_shot_roll4_sum", # ?
"home_shotontarget_per_shots_roll4",
"away_team_opponents_shot_roll4_sum", # ?
"away_team_opponents_shot_on_target_roll3_sum"
)
# random forest based selection
basic_numeric_vars_set4 <- c("odds_away_team_win",
"odds_home_team_win",
"odds_draw",
"odds_home_team_defeat_roll4_mean",
"odds_home_team_defeat_roll3_mean",
"odds_home_team_defeat_roll2_mean",
"odds_away_team_defeat_roll1_mean",
"odds_away_team_defeat_roll4_mean",
"odds_away_team_defeat_roll3_mean",
"odds_away_team_defeat_roll2_mean",
"odds_home_team_win_roll4_mean",
"odds_home_team_win_roll3_mean",
"odds_home_team_win_roll2_mean",
"odds_away_team_win_roll4_mean",
"odds_away_team_win_roll3_mean",
"odds_away_team_win_roll2_mean",
"odds_away_team_draw_roll4_mean",
"home_team_expected_point_roll4_sum",
"away_team_expected_point_roll4_sum",  # human input
"away_team_expected_point_mean",
"home_team_expected_point_mean"
)
# Log variables
# Higher order variables
# Factorized variables
# Dummy variables
# basic three odds
predictors_1 <- c(basic_numeric_vars)
# all variables
predictors_2 <- colnames(data[,-c("home_win_flag", "draw_flag", "away_win_flag", "match_id")])
# glmnet (elastic net penalized) based selection
predictors_3 <- c(basic_numeric_vars_set2)
# xgb based selection
predictors_4 <- c(basic_numeric_vars_set3)
# random forest based selection
predictors_5 <- c(basic_numeric_vars_set4)
?formula
as.formula(paste("y ~ ", paste(xnam, collapse= "+"))))
as.formula(paste("y ~ ", paste(xnam, collapse= "+")))
as.formula(paste("y ~ ", paste(predictors_1, collapse= "+"))
)
as.formula(paste("y ~ ", paste(predictors_1, collapse= "+")))
library(rstanarm)
# Fit Bayesian logistic regression model
stanglm_model <- stan_glm(as.formula(paste("home_win_flag ~ ", paste(predictors_1, collapse= "+"))),
data = data_train, family = binomial())
# Summarize the posterior distribution of coefficients
summary(stanglm_model)
# Plot the posterior distribution of coefficients
plot(stanglm_model)
predicted_stan_glm_holdout <- predict(stanglm_model,
newdata = data_holdout)
prediction_holdout_stanglm <- prediction(predicted_stan_glm_holdout, data_holdout$home_win_flag)
auc_holdout_stanglm<- performance(prediction_holdout_stanglm, measure = "auc")@y.values[[1]]
message(paste0("AUC on holdout set for STAN GLM: ", round(auc_holdout_stanglm, digits = 4)))
# Fit Bayesian logistic regression model
stanglm_model <- stan_glm(as.formula(paste("home_win_flag ~ ", paste(predictors_3, collapse= "+"))),
data = data_train, family = binomial())
predicted_stan_glm_holdout <- predict(stanglm_model,
newdata = data_holdout)
prediction_holdout_stanglm <- prediction(predicted_stan_glm_holdout, data_holdout$home_win_flag)
auc_holdout_stanglm<- performance(prediction_holdout_stanglm, measure = "auc")@y.values[[1]]
message(paste0("AUC on holdout set for STAN GLM: ", round(auc_holdout_stanglm, digits = 4)))
#  train the model
set.seed(857)
tune_grid_stanglm <- expand.grid(
alpha = seq(0, 1, 0.1)
)
# xbg feature selection is the best, but glmnet selection is very good too
glmnet_model <- train(home_win_flag ~ .,
data = data_train[ ,colnames(data_train) %in% c("home_win_flag", predictors_3), with=FALSE],
method = "stan_glm",
# family = binomial(),
trControl = train_control,
tuneGrid = tune_grid_stanglm)
# Define custom model using stan_glm
stan_glm_custom <- function(x, y, wts, param, lev, last, classProbs, ...) {
require(rstanarm)
stan_glm(
formula = as.formula(paste0("factor(", paste(lev, collapse = "+"), ") ~ .")),
data = as.data.frame(cbind(y, x)),
weights = wts,
family = binomial(),
...
)
}
# Create train control
train_control <- trainControl(
method = "cv", # cross-validation
number = 10, # number of folds
savePredictions = "final", # save predictions on holdout set
verboseIter = TRUE # print progress
)
# Train the model
model <- train(
x = data[, c("odds_home_team_win", "odds_away_team_win")],
y = factor(data[, "d_home_win_flag"]),
method = stan_glm_custom, # use the custom model
trControl = train_control,
tuneLength = 3 # number of hyperparameter combinations to try
)
# Define custom model using stan_glm
stan_glm_custom <- function(x, y, wts, param, lev, last, classProbs, ...) {
require(rstanarm)
stan_glm(
formula = as.formula(paste0("factor(", paste(lev, collapse = "+"), ") ~ .")),
data = as.data.frame(cbind(y, x)),
weights = wts,
family = binomial(),
...
)
}
# Create train control
train_control <- trainControl(
method = "cv", # cross-validation
number = 10, # number of folds
savePredictions = "final", # save predictions on holdout set
verboseIter = TRUE # print progress
)
# Train the model
model <- train(
x = data[, c("odds_home_team_win", "odds_away_team_win")],
y = factor(data[, "d_home_win_flag"]),
method = stan_glm_custom, # use the custom model
trControl = train_control,
tuneLength = 3 # number of hyperparameter combinations to try
)
# Train the model
model <- train(
x = data_train[, c("odds_home_team_win", "odds_away_team_win")],
y = factor(data_train[, "home_win_flag"]),
method = stan_glm_custom, # use the custom model
trControl = train_control,
tuneLength = 3 # number of hyperparameter combinations to try
)
# Define custom model using stan_glm
stan_glm_custom <- function(x, y, wts, param, lev, last, classProbs, ...) {
require(rstanarm)
stan_glm(
formula = as.formula(paste0("factor(", paste(lev, collapse = "+"), ") ~ .")),
data = as.data.frame(cbind(y, x)),
weights = wts,
family = binomial(),
...
)
}
# Create train control
train_control <- trainControl(
method = "cv", # cross-validation
number = 10, # number of folds
savePredictions = "final", # save predictions on holdout set
verboseIter = TRUE # print progress
)
# Train the model
model <- train(
x = data_train[, c("odds_home_team_win", "odds_away_team_win")],
y = factor(data_train[, "home_win_flag"]),
method = stan_glm_custom, # use the custom model
trControl = train_control,
tuneLength = 3 # number of hyperparameter combinations to try
)
# AUC on holdout set
predicted_bayesglm_holdout <- predict(bayesglm_model,
newdata = data_holdout,
type = "prob")[["yes"]]
