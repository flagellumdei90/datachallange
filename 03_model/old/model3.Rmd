---
title: "Data Challenge competition - Football prediction"
output: html_notebook
time: February 2023
author: Attila Gulyás / Team Bella Vita
content:Aim of this code: To build a predictive models to predict the outcome of football matches using historical data.
---

Note: 
- Data was provided by the organizer of the competition. It is found to be clean, no missing values, no duplicates.
- There are 23 divisions and 10 seasons in the data.


BASIC SETUP
```{r}
# empty memory
rm(list=ls())

# KOMMENT: install library if not exist -> megoldani majd

# Load libraries
library("tidyverse") # imap function for roc curve
library("data.table") # to use data in an efficient way
library("readxl") # to read xlsx data
library("caret") # to split data into train and holdput sets
library("dplyr") # to append commands easier
library("ROCR") # to evaluate model performance
library("h2o") # for autoML, java is kell hozzá
library("xgboost") # for extreme gradient boosting model
library("e1071") # for Naive Bayes
library("ranger") # for RF model
library("kernlab") # for SVM
library("mltools") # to calculate AUC manually
# library(ggplot2)
# library(ggthemes)
# library(grid)
# library(lattice)
# library(glmnet)
# library("skimr") 
# library(gridExtra)

# Set the working directory
dir <-  "/Users/attilagulyas/Documents/GitHub/DataChallenge2023/"

#location folders
data_in <- paste0(dir,"01_raw/")
# func <- paste0(dir, "work/") - nem használt
output <- paste0(dir, "04_output/")
```

LOAD DATA
```{r}
data <- as.data.table(read_excel(paste0(data_in, "competition_table.xlsx")))


# BASIC STISTICS
message(paste0("Döntetlen aránya: ", round(sum(data$draw_flag) / nrow(data), digits = 2)))
message(paste0("Hazai győzelem aránya: ", round(sum(data$home_win_flag) / nrow(data), digits = 2)))
message(paste0("Vendég győzelem aránya: ", round(sum(data$away_win_flag) / nrow(data), digits = 2)))
```

LABEL ENGINEERING
```{r}
data <- data %>%
  mutate(home_win_flag = factor(ifelse(home_win_flag == 0, "no", "yes"), levels = c("no", "yes")))

data <- data %>%
  mutate(away_win_flag = factor(ifelse(away_win_flag == 0, "no", "yes"), levels = c("no", "yes")))

data <- data %>%
  mutate(draw_flag = factor(ifelse(draw_flag == 0, "no", "yes"), levels = c("no", "yes")))
```

FEATURE ENGINEERING
```{r}
# data structure
str(data)

# itt hozzunk majd létre az új változókat
```

--------------------------------------
MODEL FOR PREDICTING HOME WIN
--------------------------------------

Create training set and holdout set samples
```{r}
set.seed(1990)

# smaller than usual training set so that models run faster
train_indices <- createDataPartition(data$home_win_flag, p = 0.7, list = FALSE)
data_train <- data[train_indices, ]

data_holdout <- data[-train_indices, ]

dim(data_train)
dim(data_holdout)
```

Set evaluation rules
```{r}
train_control <- trainControl(
  method = "cv", 
  n = 5,
  classProbs = TRUE, 
  summaryFunction = twoClassSummary  # necessary!
)

# MEGJ: repeated CV is egy megoldás

# Define the control parameters for the train function
# ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3, 
#                      classProbs = TRUE, summaryFunction = twoClassSummary)
```

Define variables
```{r}
# Basic numeric variables
basic_numeric_vars <- c(
  "odds_home_team_win",
  "odds_away_team_win",
  "odds_draw"
) 

# Log variables

# Higher order variables

# Factorized variables

# Dummy variables

```

Define predictor sets
```{r}

predictors_1 <- c(basic_numeric_vars)
predictors_2 <- colnames(data[,-c("home_win_flag", "draw_flag", "away_win_flag")])

```

--------------------------------------
MODELLEK:
--------------------------------------


AutoML model: baseline model
--------------------------------------
```{r}

h2o.init()
data_train_h2o <- as.h2o(data_train[,-c("away_win_flag", "draw_flag")])

# Define the target variable
target <- "home_win_flag"

# Split data into training and validation sets
splits <- h2o.splitFrame(data_train_h2o, ratios = 0.8, seed = 1234)
train <- splits[[1]]
valid <- splits[[2]]

set.seed(857)
# Run AutoML for 20 base models
aml = h2o.automl(x = predictors_1, y = target,
                 training_frame = train,
                 validation_frame = valid,
                 max_models = 20,
                 seed = 1,
                 max_runtime_secs = 20
)

# AutoML Leaderboard
lb = aml@leaderboard
lb

# close h2o connection
h2o.shutdown(prompt = F)
```


Simple logistic regression
--------------------------------------
megj: a változószelekción még dolgozni kell: 1) domain knowledge, forward/backward/stepwise selection, lasso /ridge penalty
```{r}
#  train the model
set.seed(857)
glm_model <- train(home_win_flag ~ .,
                   data = data_train[ ,colnames(data_train) %in% c("home_win_flag", predictors_1), with=FALSE],
                   method = "glm",
                   trControl = train_control)
```

```{r}
# model summary
summary(glm_model)
```

```{r}
# AUC on validation set
auc_cv <- glm_model[["results"]][["ROC"]]
message(paste0("AUC from CV: ", round(auc_cv, digits = 4)))
```

```{r}
# variable importance
varimp <- varImp(glm_model)
plot(varImp(glm_model))
```


Extreme Gradient Boosting
--------------------------------------
```{r}
#  train the model
set.seed(19900829)
tune_grid_xgb <- expand.grid(
  nrounds = c(100),  # this is n_estimators in the python code above
  max_depth = c(10),
  colsample_bytree = seq(0.5, 0.9, length.out = 5),
  ## The values below are default values in the sklearn-api. 
  eta = 0.1,
  gamma=0,
  min_child_weight = 1,
  subsample = 1
)


xgb_model <- train(
  home_win_flag ~ .,
  data = data_train[ ,colnames(data_train) %in% c("home_win_flag", predictors_1), with=FALSE],
  tuneLength = 1,
  method = 'xgbTree',
  na.action = na.omit,
  # importance = 'permutation',
  # mtry: cannot be more than the number of predictors 
  tuneGrid = tune_grid_xgb,
  trControl = train_control)

```

```{r}
# model summary
# summary(xgb)
```

```{r}
# AUC on validation set
auc_cv <- xgb_model[["results"]][["ROC"]]
message(paste0("AUC from CV: ", round(auc_cv, digits = 4)))
```

```{r}
# variable importance
varimp <- varImp(xgb_model)
plot(varImp(xgb_model))
```

A celebrated implementation of the gradient boosting idea. 
Both xgboost and gbm follows the principle of gradient boosting. There are however, the difference in modeling details. Specifically, xgboost used a more regularized model formalization to control over-fitting, which gives it better performance.


"Simple" Gradient Boosting Machine
--------------------------------------
```{r}
#  train the model
set.seed(19900829)
tune_grid_gbm <- expand.grid(n.trees = 100, 
                        interaction.depth = c(5), 
                        shrinkage = c(0.005),
                        n.minobsinnode = c(5))

gbm_model <- train(home_win_flag ~ .,
                   data = data_train[ ,colnames(data_train) %in% c("home_win_flag", predictors_1), with=FALSE], 
                   method = "gbm",
                   trControl = train_control,
                   tuneGrid = tune_grid_gbm,
                   bag.fraction = 0.8,
                   verbose = FALSE # gbm by default prints too much output
)

```

```{r}
# model summary
# summary(gbm_model)
```

```{r}
# AUC on validation set
auc_cv <- gbm_model[["results"]][["ROC"]]
message(paste0("AUC from CV: ", round(auc_cv, digits = 4)))
```

```{r}
# varimp <- varImp(gbm_model)
# plot(varImp(gbm_model))
```


Naive Bayes
--------------------------------------
```{r}
#  train the model
set.seed(19900829)

nb_model <- train(home_win_flag ~ .,
                  data = data_train[ ,colnames(data_train) %in% c("home_win_flag", predictors_1), with=FALSE],
                  method = "naive_bayes",
                  trControl = train_control, verbose = FALSE)
```

```{r}
# model summary
summary(nb_model)
```

```{r}
# AUC on validation set
auc_cv <- nb_model[["results"]][["ROC"]]
message(paste0("AUC from CV: ", round(auc_cv, digits = 4)))
```

```{r}
varimp <- varImp(nb_model)
plot(varImp(nb_model))
```


Random Forest
--------------------------------------
```{r}
#  train the model
set.seed(19900829)
tune_grid_rf <- expand.grid(
  .mtry = 3,
  .splitrule = "gini",
  .min.node.size = c(5) # implicitly sets the depth of your trees
)


rf_model <- train(
  home_win_flag ~ .,
  data = data_train[ ,colnames(data_train) %in% c("home_win_flag", predictors_1), with=FALSE],
  tuneLength = 1,
  method = 'ranger',
  na.action = na.omit,
  importance = 'permutation',
  # mtry: cannot be more than the number of predictors 
  tuneGrid = tune_grid_rf,
  trControl = train_control)
```

```{r}
# model summary
summary(rf_model)
```

```{r}
# AUC on validation set
auc_cv <- rf_model[["results"]][["ROC"]]
message(paste0("AUC from CV: ", round(auc_cv, digits = 4)))
```

```{r}
varimp <- varImp(rf_model)
plot(varImp(rf_model))
```

SVM
--------------------------------------
```{r}
#  train the model
set.seed(19900829)
tune_grid_svm <- expand.grid(
  .C = 5
)


svm_model <- train(
  home_win_flag ~ .,
  data = data_train[ ,colnames(data_train) %in% c("home_win_flag", predictors_1), with=FALSE],
  tuneLength = 1,
  method = 'svmLinear',
  na.action = na.omit,
  importance = 'permutation',
  # mtry: cannot be more than the number of predictors 
  tuneGrid = tune_grid_svm,
  trControl = train_control)
```

```{r}
# model summary
summary(svm_model)
```

```{r}
# AUC on validation set
auc_cv <- svm_model[["results"]][["ROC"]]
message(paste0("AUC from CV: ", round(auc_cv, digits = 4)))
```

```{r}
varimp <- varImp(svm_model)
plot(varImp(svm_model))
```


K-Nearest Neighbours
--------------------------------------
```{r}
#  train the model
set.seed(19900829)
tune_grid_knn <- expand.grid(
  .k= 5
)


knn_model <- train(
  home_win_flag ~ .,
  data = data_train[ ,colnames(data_train) %in% c("home_win_flag", predictors_1), with=FALSE],
  tuneLength = 1,
  method = 'knn',
  na.action = na.omit,
  # importance = 'permutation',
  # mtry: cannot be more than the number of predictors 
  tuneGrid = tune_grid_knn,
  trControl = train_control)
```

```{r}
# model summary
summary(knn_model)
```

```{r}
# AUC on validation set
auc_cv <- knn_model[["results"]][["ROC"]]
message(paste0("AUC from CV: ", round(auc_cv, digits = 4)))
```

```{r}
varimp <- varImp(knn_model)
plot(varImp(knn_model))
```

ENSEMBLED MODEL
--------------------------------------
```{r}
h2o.init()
data_train_h2o <- as.h2o(data_train[,-c("away_win_flag", "draw_flag")])

# Identify predictors and response
y <- "home_win_flag"
x <- predictors_1

# Train & Cross-validate a GBM
my_gbm <- h2o.gbm(x = x,
                  y = y,
                  training_frame = data_train_h2o,
                  nfolds = 5,
                  keep_cross_validation_predictions = TRUE,
                  seed = 5)

# Train & Cross-validate a RF
my_rf <- h2o.randomForest(x = x,
                          y = y,
                          training_frame = data_train_h2o,
                          nfolds = 5,
                          keep_cross_validation_predictions = TRUE,
                          seed = 5)
# Train & Cross-validate a LR
my_lr <- h2o.glm(x = x,
                 y = y,
                 training_frame = data_train_h2o,
                 family = c("binomial"),
                 nfolds = 5,
                 keep_cross_validation_predictions = TRUE,
                 seed = 5)
# Train a stacked random forest ensemble using the GBM, RF and LR above
ensemble <- h2o.stackedEnsemble(x = x,
                                y = y,
                                metalearner_algorithm="drf",
                                training_frame = data_train_h2o,
                                base_models = list(my_gbm, my_rf, my_lr))
```

```{r}
# model summary
summary(ensemble)
```


--------------------------------------
EVALUATION: Compare random forests models based on VALIDATION sets 
--------------------------------------
```{r}

results <- resamples(
  list(
    #model_1 = aml,
    GLM = glm_model,
    XGB = xgb_model,
    GMB = gbm_model,
    NB = nb_model,
    RF = rf_model,
    SVM = svm_model,
    KNN = knn_model
    #model_9 = ensemble
  )
)
summary(results)
```

--------------------------------------
EVALUATION: Compare random forests models based on TEST set
--------------------------------------

Logistic regression
```{r}
# AUC on holdout set
predicted_glm_holdout <- predict(glm_model, 
                                 newdata = data_holdout, 
                                 type = "prob")[["yes"]]


prediction_holdout_glm <- prediction(predicted_glm_holdout, data_holdout$home_win_flag)
auc_holdout_glm <- performance(prediction_holdout_glm, measure = "auc")@y.values[[1]]
message(paste0("AUC on holdout set for GLM: ", round(auc_holdout_glm, digits = 4)))
```

Random forest
```{r}
# AUC on holdout set
predicted_rf_holdout <- predict(rf_model, 
                         newdata = data_holdout, 
                         type = "prob")[["yes"]]


prediction_holdout_rf <- prediction(predicted_rf_holdout, data_holdout$home_win_flag)
auc_holdout_rf <- performance(prediction_holdout_rf, measure = "auc")@y.values[[1]]
message(paste0("AUC on holdout set for RF: ", round(auc_holdout_rf, digits = 4)))
```

XGBoosting
```{r}
# AUC on holdout set
predicted_xgb_holdout <- predict(xgb_model, 
                         newdata = data_holdout, 
                         type = "prob")[["yes"]]


prediction_holdout_xgb <- prediction(predicted_xgb_holdout, data_holdout$home_win_flag)
auc_holdout_xgb <- performance(prediction_holdout_xgb, measure = "auc")@y.values[[1]]
message(paste0("AUC on holdout set for XGB: ", round(auc_holdout_xgb, digits = 4)))
```

Gradient Boosting Machine
```{r}
# AUC on holdout set
predicted_gbm_holdout <- predict(gbm_model, 
                         newdata = data_holdout, 
                         type = "prob")[["yes"]]


prediction_holdout_gbm <- prediction(predicted_gbm_holdout, data_holdout$home_win_flag)
auc_holdout_gbm <- performance(prediction_holdout_gbm, measure = "auc")@y.values[[1]]
message(paste0("AUC on holdout set for GBM: ", round(auc_holdout_gbm, digits = 4)))
```

Naive Bayes
```{r}
# AUC on holdout set
predicted_nb_holdout <- predict(nb_model, 
                         newdata = data_holdout, 
                         type = "prob")[["yes"]]


prediction_holdout_nb <- prediction(predicted_nb_holdout, data_holdout$home_win_flag)
auc_holdout_nb <- performance(prediction_holdout_nb, measure = "auc")@y.values[[1]]
message(paste0("AUC on holdout set for NB: ", round(auc_holdout_nb, digits = 4)))
```

SVM
```{r}
# AUC on holdout set
predicted_svm_holdout <- predict(svm_model, 
                         newdata = data_holdout, 
                         type = "prob")[["yes"]]


prediction_holdout_svm <- prediction(predicted_svm_holdout, data_holdout$home_win_flag)
auc_holdout_svm <- performance(prediction_holdout_svm, measure = "auc")@y.values[[1]]
message(paste0("AUC on holdout set for SVM: ", round(auc_holdout_svm, digits = 4)))
```

KNN
```{r}
# AUC on holdout set
predicted_knn_holdout <- predict(knn_model, 
                         newdata = data_holdout, 
                         type = "prob")[["yes"]]


prediction_holdout_knn <- prediction(predicted_knn_holdout, data_holdout$home_win_flag)
auc_holdout_knn <- performance(prediction_holdout_knn, measure = "auc")@y.values[[1]]
message(paste0("AUC on holdout set for KNN: ", round(auc_holdout_knn, digits = 4)))
```

ODDS based probability
--------------------------------------
```{r}
data_holdout[,.prob_home_win_odds_based := 1/(1+odds_home_team_win)]
data_holdout[,.prob_draw_odds_based := 1/(1+odds_draw)]
data_holdout[,.prob_away_win_odds_based := 1/(1+odds_away_team_win)]

data_holdout[,.prob_sum_odds_based := .prob_home_win_odds_based + .prob_draw_odds_based + .prob_away_win_odds_based]

data_holdout[,.prob_home_win_odds_based_adj := .prob_home_win_odds_based / .prob_sum_odds_based]
data_holdout[,.prob_draw_odds_based_adj := .prob_draw_odds_based / .prob_sum_odds_based]
data_holdout[,.prob_away_win_odds_based_adj := .prob_away_win_odds_based / .prob_sum_odds_based]

```


COLLECT PREDICTIONS INTO A OUTPUT TABLE
```{r}
predcition_lists <-c("GLM" = prediction_holdout_glm@predictions,
                     "GBM" = prediction_holdout_gbm@predictions,
                     "XGB" = prediction_holdout_xgb@predictions,
                     "RF" = prediction_holdout_rf@predictions,
                     "NB" = prediction_holdout_nb@predictions,
                     "KNN" = prediction_holdout_knn@predictions,
                     "SVM" = prediction_holdout_svm@predictions)
prediction_table <-data.table(do.call(cbind, predcition_lists))

# model average
prediction_table$mean = (prediction_table$GLM + prediction_table$GBM + prediction_table$XGB + prediction_table$RF + prediction_table$NB + prediction_table$KNN + prediction_table$SVM ) / 7

# odds based
prediction_table$odds_based <- data_holdout$.prob_home_win_odds_based_adj

# Results - fact
prediction_table$actual <- data_holdout$home_win_flag
```

```{r}
# Calculate AUC manually

auc_holdout_mean <- auc_roc(preds = prediction_table$mean, actuals = ordered(prediction_table$actual))
auc_holdout_oddsbased<- auc_roc(preds = prediction_table$odds_based, actuals = ordered(prediction_table$actual))

message(paste0("AUC on holdout set for Mean Model: ", round(auc_holdout_mean, digits = 4)))
message(paste0("AUC on holdout set for ODDS based Model: ", round(auc_holdout_oddsbased, digits = 4)))

```

ROC curve on the TEST set 
```{r}

roc_df_logit_rf <- imap(list("random forest" = rf_model, 
                             "GLM" = glm_model,
                             "GBM" = gbm_model,
                             "XGB" = xgb_model,
                             "NB" = nb_model,
                             "KNN" = knn_model,
                             "SVM" = svm_model
                             ), ~ {
  model <- .x
  predicted_probabilities <- predict(model, newdata = data_holdout, type = "prob")
  rocr_prediction <- prediction(predicted_probabilities[["yes"]], data_holdout[["home_win_flag"]]) 
  
  tpr_fpr_performance <- performance(rocr_prediction, "tpr", "fpr")
  tibble(
    model = .y,
    FPR = tpr_fpr_performance@x.values[[1]],
    TPR = tpr_fpr_performance@y.values[[1]],
    cutoff = tpr_fpr_performance@alpha.values[[1]]
  )  
}) %>% bind_rows()

ggplot(roc_df_logit_rf) +
  geom_line(aes(FPR, TPR, color = model), size = 1) +
  geom_abline(intercept = 0, slope = 1,  linetype = "dotted", col = "black") +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .1)) +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, .1)) +
  xlab("False Positive Rate") + ylab("True Positive Rate") +
  ggtitle("ROC curve, calculated on the test set")

```

SAVE MODELS
```{r}
saveRDS(rf_model, paste0(output,"home_model_rf.rda"))
saveRDS(glm_model, paste0(output,"home_model_glm.rda"))
saveRDS(gbm_model, paste0(output,"home_model_gbm.rda"))
saveRDS(xgb_model, paste0(output,"home_model_xgb.rda"))
saveRDS(nb_model, paste0(output,"home_model_nb.rda"))
saveRDS(knn_model, paste0(output,"home_model_knn.rda"))
saveRDS(svm_model, paste0(output,"home_model_svm.rda"))
```

LOAD MODELS
```{r}
home_model_rf <- readRDS(paste0(output,"home_model_rf.rda"))
home_model_glm <- readRDS(paste0(output,"home_model_glm.rda"))
home_model_gbm <- readRDS(paste0(output,"home_model_gbm.rda"))
home_model_xgb <- readRDS(paste0(output,"home_model_xgb.rda"))
home_model_nb <- readRDS(paste0(output,"home_model_nb.rda"))
home_model_knn <- readRDS(paste0(output,"home_model_knn.rda"))
home_model_svm <- readRDS(paste0(output,"home_model_svm.rda"))
```

TRY LOADED MODEL
```{r}
# AUC on holdout set
predicted_proba_holdout <- predict(home_model_rf, 
                         newdata = data_holdout, 
                         type = "prob")[["yes"]]


prediction_holdout_proba <- prediction(predicted_proba_holdout, data_holdout$home_win_flag)
auc_holdout_proba <- performance(prediction_holdout_proba, measure = "auc")@y.values[[1]]
message(paste0("AUC on holdout set for RF: ", round(auc_holdout_proba, digits = 4)))
```


CALIBRATION TEST
```{r}
# calibration plot: how well do estimated vs actual event probabilities relate to each other?
# -------------------------------------------------
actual_vs_predicted_rf1 <- data.frame(
  actual = ifelse(data_holdout[["home_win_flag"]] == "yes", 1, 0),
  predicted = predicted_rf1_holdout
)

# order observations accordingly

num_groups <- 10

calibration_logit <- actual_vs_predicted_rf1 %>% 
  # ntile: assign group ids based on a variable. similar to "cut".
  mutate(predicted_score_group = ntile(predicted, num_groups)) %>% 
  group_by(predicted_score_group) %>% 
  summarise(mean_actual = mean(actual), mean_predicted = mean(predicted), num_obs = n())

ggplot(calibration_logit,aes(x = mean_actual, y = mean_predicted)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed") +
  labs(x = "Actual event probability", y = "Predicted event probability") +
  ylim(0, 1) + xlim(0, 1)


# the Brier score: MSE applied to classification evaluation

brier <- RMSE(actual_vs_predicted_rf1[["predicted"]], actual_vs_predicted_rf1[["actual"]])^2
message(paste0("Brier score: ", round(brier, digits = 4)))
```

